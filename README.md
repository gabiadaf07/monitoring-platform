**Acest schelet de proiect si acest README.MD sunt orientative.** 
**Aveti libertatea de a aduga alte fisiere si a modifica acest schelet cum doriti. Important este sa implementati proiectul conform cerintelor primite.**
**Acest text si tot textul ajutator de mai jos trebuiesc sterse inainte de a preda proiectul.**

**Pentru a clona acest proiect creati propriul vostru proiect EMPTY in gihub si rulati:**
```bash
git clonegit@github.com:gabiadaf07/monitoring-platform.git
```


# Platforma de Monitorizare a Starii unui Sistem

## Scopul Proiectului
- [Proiectul este un monitoring-platform ce poate deservi la monitorizarea sistemelor si utilizarea eficienta a resurselor hardware , pentru a maximiza durata de functionare a aplicatiilor fara a exista perioade de downtime ale aplicatiilor. ]

### Arhitectura proiectului
Acest subpunct este BONUS.
- [Desenati in excalidraw sau in orice tool doriti arhitectura generala a proiectului si includeti aici poza cu descrierea pasilor]

- Acesta este un exemplu de inserare de imagine in README.MD. Puneti imagine in directorul de imagini si o inserati asa:

![Jenkins Logo](imagini/jenkins-logo.png)

Consultati si [Sintaxa Markdown](https://www.markdownguide.org/cheat-sheet/)

## Structura Proiectului
[Aici descriem rolul fiecarui director al proiectului. Descrierea trebuie sa fie foarte pe scurt la acest pas. O sa intrati in detalii la pasii urmatori.]
- `/scripts`: [Aici avem directorul de scripts: unde avem scriptul de backup.py si scriptul de monitoring.sh]
- `/docker`: [Descriere Dockerfiles È™i docker-compose.yml . Avem 3 imagini Docker :
    - imaginea pentru partea de backup care copiaza, instaleaza librariile necesare si variabilele de mediu .
    - imaginea pentru partea de monitoring care copiaza fisierul de bash, asigura drept de executie a fisierului si ruleaza fisierul monitoring.sh
    - imaginea pentru jenkins pentru create containerului de jenkins pentru a rula pipeline-urile]
- `/ansible`: [Descriere rolurilor playbook-urilor È™i inventory:
    - install_docker.yml reprezinta un playbook ce automatizeazÄƒ procesul de instalare a Docker pe o maÈ™inÄƒ virtualÄƒ remote, asigurÃ¢nd configurarea corectÄƒ a surselor, pachetelor È™i permisiunilor necesare.
    - deploy_playform.yml reprezinta un playbook ce automatizeazÄƒ
    procesul de instalare a docker compose.
- `/jenkins`: [Aici regasim folderul pentru jenkins in care avem]:
    `-/pipelines`: [Folderul de pipelines deserveste la urmatoarele doua pipeline-uri : ]
        - `/backup`: Acest Jenkinsfile defineÈ™te un pipeline complet pentru gestionarea aplicaÈ›iei de backup, folosind containere Docker È™i integrare cu Docker Hub. Pipeline-ul este Ã®mpÄƒrÈ›it Ã®n mai multe etape care acoperÄƒ verificarea codului, construirea imaginii Docker, publicarea acesteia È™i lansarea aplicaÈ›iei.
        -`/monitoring`:  Acest Jenkinsfile defineÈ™te un pipeline complet pentru gestionarea aplicaÈ›iei de monitoring, folosind containere Docker È™i integrare cu Docker Hub. Pipeline-ul este Ã®mpÄƒrÈ›it Ã®n mai multe etape care acoperÄƒ  construirea imaginii Docker, publicarea acesteia È™i lansarea aplicaÈ›iei.
- `/terraform`: [Descriere rol fiecare fisier Terraform folosit]

    - main.tf : FiÈ™ierul principal care defineÈ™te resursele infrastructurii. Aici sunt declarate instanÈ›ele EC2, grupurile de securitate, bucket-urile S3, key pair-urile È™i orice altÄƒ resursÄƒ AWS sau LocalStack.
    - backend.tf : ConfigureazÄƒ backend-ul Terraform â€” adicÄƒ locul unde se salveazÄƒ fiÈ™ierul `terraform.tfstate`. 
    - variables.tf : ConÈ›ine toate variabilele de intrare utilizate Ã®n proiect. Aici sunt definite valorile parametrizabile precum `ami_id`, `instance_type`, `region`, etc. Permite reutilizarea È™i flexibilitatea configuraÈ›iei.
    - outputs.tf : DefineÈ™te valorile care vor fi afiÈ™ate dupÄƒ rularea comenzii `terraform apply`. De exemplu, IP-ul instanÈ›ei EC2, numele bucketului S3 sau orice altÄƒ informaÈ›ie utilÄƒ pentru paÈ™ii urmÄƒtori (Ansible, Jenkins, etc.).
    - versions : SpecificÄƒ versiunea minimÄƒ de Terraform necesarÄƒ .
         



## Setup È™i Rulare
- [InstrucÈ›iuni de setup local È™i remote. Aici trebuiesc puse absolut toate informatiile necesare pentru a putea instala si rula proiectul. De exemplu listati aici si ce tool-uri trebuiesc instalate (Ansible, SSH config, useri, masini virtuale noi daca este cazul, etc) pasii de instal si comenzi].
- [Dependinte necesare]
    ```bash
        # Terraform
    sudo apt update && sudo apt install -y terraform

    # Ansible
    sudo apt install -y ansible

    # Docker & Docker Compose
    sudo apt install -y docker.io
    sudo curl -L "https://github.com/docker/compose/releases/download/v2.24.5/docker-compose-linux-x86_64" -o /usr/local/bin/docker-compose
    sudo chmod +x /usr/local/bin/docker-compose

    # Python (pentru scripturi)
    sudo apt install -y python3 python3-pip
    ```
- [Creeare cheie SSH :] 
```bash
    ssh-keygen -t rsa -f ~/.ssh/ansible_key
    cat /home/admin0103/.ssh/ansible_key.pub
```

- [Instalarea serviciului ssh pe masina remote  sau masina 2]

```bash
su - (ne logÄƒm ca root)
apt update
apt install -y openssh-server
systemctl status sshd

```

- [AdÄƒugaÈ›i un user nou cu numele eu pe maÈ™ina 2 si adÄƒugati cheia lui publicÄƒ generatÄƒ anterior (la pasul 1) Ã®n lista de chei autorizate.]

```bash
adduser theansible
su - theansible
mkdir ~/.ssh
echo public_key >> ~/.ssh/authorized_keys
chmod -R go= ~/.ssh  
ls -ld ~/.ssh
```
- [Observatii: ]
- [Ãn loc de public_key trebuie sÄƒ puneÈ›i Ã®ntre ghilimele cheia publicÄƒ generatÄƒ la pasul 1.]
- [Ãn Linux, directorul .ssh È™i conÈ›inutul sÄƒu trebuie sÄƒ aibÄƒ drepturi restricÈ›ionate pentru group È™i others. DacÄƒ nu sunt restricÈ›ionate, protocolul SSL nu le ia Ã®n considerare din motive de securitate]

- [Pasul X: ]
    - SetaÈ›i un network bridge Ã®ntre cele douÄƒ maÈ™ini pentru a putea accesa maÈ™ina 2 (remote) din masina 1 (Ubuntu2204). Pentru a face acest lucru trebuie sÄƒ: 
    opriÈ›i ambele maÈ™ini (cu shutdown, nu save state) 
    pentru fiecare maÈ™inÄƒ
    daÈ›i click pe Settings > Network > Adapter 2 (NU modificati Adapter 1. Acela este folosit pentru accesul la internet al maÈ™inilor)
    Bifati Enable Network Adapter
    Selectati Attached to: Bridge Adapter È™i daÈ›i click pe ok
    DupÄƒ ce aÈ›i fÄƒcut acest lucru pentru ambele masini, le daÈ›i start]

- [Pasul Y: ]
    - [VerificaÈ›i IP-ul maÈ™inii remote folosind comanda:]
```bash
ip addr
```
    [- IP-ul cÄƒutat este cel mai de jos,  din dreptul adresei inet.]

-   [!Inet Imagine ]
- [Pasul Z:]
    - [ExecutaÈ›i comanda de ssh de pe maÈ™ina 1 folosind IP-ul obÈ›inut mai sus:
    DacÄƒ setup-ul a fost fÄƒcut cu success, ar trebui sÄƒ vedeÈ›i un rezultat similar cu acesta:
    ]
- [!inet Imagine 2]
- [Descrieti cum ati pornit containerele si cum ati verificat ca aplicatia ruleaza corect.]

- [Includeti aici pasii detaliati de configurat si rulat Ansible pe masina noua]
    Intrati pe masina noua si folositi comanda folosita la pasul anterior pentru a vedea IP-ul masinii remote.
    Folositi ip-ul masinii remote si adaugati-l in fisierul inventory.ini din proiect din folder-ul ansible .

- [!ip inventory ini]
- [Descrieti cum verificam ca totul a rulat cu succes? Cateva comenzi prin care verificam ca Ansible a instalat ce trebuia]

- [Exemplu de fisier inventory.ini]
```yml
[dockerhost]
54.214.59.124 ansible_user=eu ansible_ssh_private_key_file=~/.ssh/id_rsa_jenkins
```
```bash
ansible-playbook -i inventory.ini install_docker.yml
ansible-playbook -i inventory.ini deploy_platform.yml
```
- [Dupa rularea comenzilor , intrati in masina remote si executati comanda :]
```bash
docker ps
docker logs
```
- [Aici veti putea observa daca procesele docker ruleaza :]
- [Verificarea fisierelor copiate : ]
```bash
ls -l /home/eu/docker/
ls -l /home/eu/docker/scripts/
```

## Setup È™i Rulare in Kubernetes
- [Comenezi pentru rularea aplicatiei in Kubernetes]
```bash
#!/bin/bash

set -e

# 1. ActivÄƒm mediul Docker din Minikube
echo "ğŸ”§ Activare mediu Docker Minikube..."
eval $(minikube docker-env)

# 2. Build imagini locale
echo "ğŸ³ Construim imaginile Docker..."
docker build -t local/backup:latest -f docker/backup/Dockerfile .
docker build -t local/monitoring:latest -f docker/monitoring/Dockerfile .

# 3. AplicÄƒm manifestele Kubernetes
echo "ğŸš€ AplicÄƒm manifestele Kubernetes..."
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/nginx-service.yaml
kubectl apply -f k8s/hpa.yaml

# 4. VerificÄƒm statusul
echo "ğŸ“¦ Poduri Ã®n namespace-ul 'monitoring':"
kubectl get pods -n monitoring

echo "ğŸŒ Servicii disponibile:"
kubectl get svc -n monitoring

echo "ğŸ“ˆ HPA status:"
kubectl get hpa -n monitoring
```

## CI/CD È™i Automatizari
- [Descriere pipeline-uri Jenkins. Puneti aici cat mai detaliat ce face fiecare pipeline de jenkins cu poze facute la pipeline in Blue Ocean. Detaliati cat puteti de mult procesul de CI/CD folosit.]

ğŸ“¦ Etape detaliate
1. Checkout
	â€¢ Preia codul sursÄƒ din repository-ul Git configurat Ã®n Jenkins.
2. Lint
	â€¢ RuleazÄƒ un container Python (python:3.12-slim) care:
		â—‹ InstaleazÄƒ flake8
		â—‹ RuleazÄƒ analiza staticÄƒ (linting) pe toate fiÈ™ierele .py
	â€¢ Scopul este sÄƒ detecteze erori de stil sau sintaxÄƒ Ã®nainte de build.
3. (ComentatÄƒ) Unit Tests
	â€¢ Etapa este momentan dezactivatÄƒ.
	â€¢ Ar rula testele unitare cu pytest Ã®ntr-un container Python.
4. Build Docker Image
	â€¢ ConstruieÈ™te imaginea Docker pentru backup folosind fiÈ™ierul docker/backup/Dockerfile.
	â€¢ EticheteazÄƒ imaginea cu gabiadaf07/backup:latest.
5. Push to Docker Hub
	â€¢ PublicÄƒ imaginea Docker Ã®n Docker Hub folosind credentialele stocate Ã®n Jenkins (DOCKER_CREDENTIALS_ID).
6. Deploy
	â€¢ RuleazÄƒ docker-compose pentru a lansa aplicaÈ›ia definitÄƒ Ã®n docker/docker-compose.yml.
ğŸ” Variabile de mediu
groovy
environment {
    DOCKER_IMAGE = 'gabiadaf07/backup:latest'
    DOCKER_CREDENTIALS_ID = 'credentiale-dockerhub'
}
ğŸ§  Scopul pipeline-ului
	â€¢ AutomatizeazÄƒ verificarea calitÄƒÈ›ii codului
	â€¢ ConstruieÈ™te È™i publicÄƒ imaginea de backup
LanseazÄƒ aplicaÈ›ia Ã®ntr-un mediu Docker orchestrat cu docker-compose




ğŸ“Š Ce face pipeline-ul Jenkins pentru monitoring
Acest Jenkinsfile automatizeazÄƒ procesul de verificare, construire, publicare È™i lansare a serviciului de monitorizare Ã®ntr-un mediu Docker orchestrat cu docker-compose.

ğŸ§© Etape ale pipeline-ului
1. Checkout
Preia codul sursÄƒ din repository-ul Git configurat Ã®n Jenkins.

2. Lint
RuleazÄƒ un container python:3.12-slim care:

InstaleazÄƒ flake8

RuleazÄƒ analiza staticÄƒ pe fiÈ™ierele .py din proiect

Scopul este sÄƒ detecteze erori de stil sau sintaxÄƒ Ã®n codul de monitorizare.

3. (OpÈ›ionalÄƒ) Unit Tests
Etapa este comentatÄƒ, dar poate fi activatÄƒ pentru a rula teste automate cu pytest.

4. Build Docker Image
ConstruieÈ™te imaginea Docker pentru serviciul de monitorizare folosind fiÈ™ierul docker/monitoring/Dockerfile.

EticheteazÄƒ imaginea cu ceva de genul gabiadaf07/monitoring:latest (dacÄƒ este configurat astfel).

5. Push to Docker Hub
PublicÄƒ imaginea Ã®n Docker Hub folosind credentialele definite Ã®n Jenkins (DOCKER_CREDENTIALS_ID).

6. Deploy
RuleazÄƒ docker-compose pentru a lansa serviciul de monitorizare Ã®mpreunÄƒ cu celelalte componente (ex: backup, nginx).

ğŸ” Variabile de mediu (exemplu)
groovy
environment {
    DOCKER_IMAGE = 'gabiadaf07/monitoring:latest'
    DOCKER_CREDENTIALS_ID = 'credentiale-dockerhub'
}
ğŸ§  Scopul pipeline-ului
AutomatizeazÄƒ verificarea calitÄƒÈ›ii codului

ConstruieÈ™te È™i publicÄƒ imaginea de monitorizare

LanseazÄƒ aplicaÈ›ia Ã®ntr-un mediu Docker orchestrat

Poate fi extins cu testare, validare È™i notificÄƒri

- [Detalii cu restul cerintelor de CI/CD (cum ati creat userul nou ce are access doar la resursele proiectului, cum ati creat un View now pentru proiect, etc)]
imagini

- [Daca ati implementat si punctul E optional atunci detaliati si setupul de minikube.]


## Terraform È™i AWS
- [Prerequiste]
- [InstrucÈ›iuni pentru rularea Terraform È™i configurarea AWS]
âš™ï¸ - [Setup LocalStack + Terraform
ğŸ§© 1. Configurare AWS CLI
bash
aws configure
Ai introdus:
	â€¢ Access Key ID: test
	â€¢ Secret Access Key: test
	â€¢ Region: us-east-1
	â€¢ Output format: json
	Aceste valori sunt standard pentru LocalStack È™i nu necesitÄƒ autentificare realÄƒ.
ğŸª£ 2. Creare bucket S3 Ã®n LocalStack
bash
aws --endpoint-url=http://localhost:4566 s3 mb s3://terraform-state-dev
	CreeazÄƒ bucket-ul terraform-state-dev Ã®n LocalStack, care va fi folosit ca backend pentru fiÈ™ierul terraform.tfstate.
ğŸ“¦ 3. IniÈ›ializare Terraform
bash
terraform init
	IniÈ›ializeazÄƒ directorul Terraform, descarcÄƒ providerii È™i configureazÄƒ backend-ul (dacÄƒ ai backend.tf).
âœ… 4. Validare configuraÈ›ie
bash
terraform validate]
ğŸ“ 4.1 Planificare infrastructurÄƒ
bash
terraform plan
	GenereazÄƒ un plan detaliat cu toate acÈ›iunile pe care Terraform le va executa. Nu modificÄƒ nimic Ã®ncÄƒ.
ğŸš€ 5. Aplicare infrastructurÄƒ
bash
terraform apply
	AplicÄƒ modificÄƒrile È™i creeazÄƒ resursele definite Ã®n fiÈ™ierele .tf. Vei fi Ã®ntrebat sÄƒ confirmi cu yes.
ğŸ“¤ 6. Verificare resurse create
DupÄƒ aplicare, poÈ›i verifica:
bash
terraform output

